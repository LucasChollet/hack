<!doctype html>
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta charset="utf-8">
<title>webcam</title>

<style>
@media(prefers-color-scheme:dark) {
  body {
    color:#E8EAED;
    background:#202124;
  }
}

video {
  display: none;
}

body {
  margin: 0;
}
canvas {
  width: 100vw;
  height: 100vh;
  display: block;
}
</style>

<video playsinline></video>
<canvas width="800" height="600"></canvas>

<script>

let isVideoLoaded = false;
let uvsNeedRecomputation = false;

// https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia#width_and_height
// For `video: true`, the built-in camera for my MBP returns a 640x480 stream,
// with a 4:3 aspect ratio. With `video: { height: 720 }`, it instead returns
// a 1280x720 stream, with a 16:9 video. With an external a7iv connected as
// webcam, this gets a 1920x1080 stream (16:9 aspect) with `video: true`.
// But with `video: { height: 720 }`, it gets the 1280x720 stream from the
// built-in camera (!).
// With `video: { height: 1080 }`, this gets 1920x1080 from the built-in
// camera if the a7iv isn't connected, but 1920x1080 from the a7iv if it is
// connected.
// With `video: { height: 2160 }`, this gets 1080x1920 (9:16) (!) from the
// built-in camera no matter if the a7iv is connected or not.
// This is weird in itself, but the non-reliable aspect ratio also means that
// for things not to look stretched on the canvas element, we either have to
// resize it to have an aspect ration matching the video's aspect ratio,
// or clip the the video element a bit on the sides (or bottom/top).
navigator.mediaDevices
  .getUserMedia({ video: true })
  .then((mediaStream) => {
    const video = document.querySelector('video');
    video.srcObject = mediaStream;
    video.onloadedmetadata = () => {
      function gcd(a, b) { return b ? gcd(b, a %b ) : a; }
      const n = gcd(video.videoWidth, video.videoHeight);
      console.log(`video size: ${video.videoWidth} x ${video.videoHeight} ` +
                  `(${video.videoWidth / n}:${video.videoHeight / n})`);
      video.play();
      isVideoLoaded = true;
      uvsNeedRecomputation = true;
      requestAnimationFrame(animation_frame);
    };
  })
  .catch((err) => {
    console.error(`${err.name}: ${err.message}`);
  });

function compileShader(gl, shaderType, shaderSource) {
  const shader = gl.createShader(shaderType);
  gl.shaderSource(shader, shaderSource);
  gl.compileShader(shader);
  if (gl.getShaderParameter(shader, gl.COMPILE_STATUS))
    return shader;

  console.error(`failed to compile shader: ${gl.getShaderInfoLog(shader)}`);
  gl.deleteShader(shader);
}

function linkProgram(gl, vertexShader, fragmentShader) {
  const program = gl.createProgram();
  gl.attachShader(program, vertexShader);
  gl.attachShader(program, fragmentShader);
  gl.linkProgram(program);
  if (gl.getProgramParameter(program, gl.LINK_STATUS))
    return program;

  console.error(`failed to link program: ${gl.getProgramInfoLog(program)}`);
  gl.deleteProgram(program);
}

function compileProgram(gl, vertexShaderSource, fragShaderSource) {
  const vertexShader = compileShader(gl, gl.VERTEX_SHADER, vertexShaderSource);
  const fragShader = compileShader(gl, gl.FRAGMENT_SHADER, fragShaderSource);
  return linkProgram(gl, vertexShader, fragShader);
}

let draw;

function setUpGl() {
  const canvas = document.querySelector('canvas');
  const gl = canvas.getContext('webgl2', {alpha: false});
  if (gl === null) {
    console.error('failed to create webgl context');
    return;
  }

  // Doesn't handle devicePixelRatio for now. That's a bit of a mess:
  // https://webglfundamentals.org/webgl/lessons/webgl-resizing-the-canvas.html
  const observer = new ResizeObserver((entries) => {
    canvas.width = canvas.clientWidth;
    canvas.height = canvas.clientHeight;
    console.log(`new canvas size: ${canvas.width} x ${canvas.height}`);

    gl.viewport(0, 0, canvas.width, canvas.height);
    uvsNeedRecomputation = true;
    draw(0);
  });
  observer.observe(canvas)

  const vertexShaderSource = `#version 300 es
    in vec2 a_position;
    in vec2 a_uv;
    out vec2 v_uv;
    void main() {
      v_uv = a_uv;
      v_uv.x = 1.0 - v_uv.x;  // Flip horizontally.
      gl_Position = vec4(a_position, 0.0, 1.0);
    }
  `;
  const fragmentShaderSource = `#version 300 es
    precision highp float;
    uniform sampler2D u_image;
    in vec2 v_uv;
    out vec4 outColor;
    void main() {
      outColor = texture(u_image, v_uv);
      outColor.rg *= v_uv;
    }
  `;

  const program = compileProgram(gl, vertexShaderSource, fragmentShaderSource);
  const positionAttributeLoc = gl.getAttribLocation(program, 'a_position');
  const uvAttributeLoc = gl.getAttribLocation(program, 'a_uv');
  const imageAttributeLoc = gl.getAttribLocation(program, 'u_image');

  const triVao = gl.createVertexArray();
  gl.bindVertexArray(triVao);

  const allCoords = [
    -1, -1,
     1, -1,
    -1,  1,
     1,  1,
  ];
  const positionBuf = gl.createBuffer();
  gl.bindBuffer(gl.ARRAY_BUFFER, positionBuf);
  gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(allCoords), gl.STATIC_DRAW);
  gl.enableVertexAttribArray(positionAttributeLoc);
  gl.vertexAttribPointer(
      positionAttributeLoc, /*size=*/2, gl.FLOAT, /*normalize=*/false,
      /*stride=*/0, /*offset=*/0);

  let uvs = [
    0, 0,
    1, 0,
    0, 1,
    1, 1,
  ];
  const uvBuf = gl.createBuffer();
  gl.bindBuffer(gl.ARRAY_BUFFER, uvBuf);
  gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(uvs), gl.STATIC_DRAW);
  gl.enableVertexAttribArray(uvAttributeLoc);
  gl.vertexAttribPointer(
      uvAttributeLoc, /*size=*/2, gl.FLOAT, /*normalize=*/false,
      /*stride=*/0, /*offset=*/0);

  gl.useProgram(program);

  let texture = gl.createTexture();
  gl.bindTexture(gl.TEXTURE_2D, texture);

  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);

  gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, true);
  gl.texImage2D(gl.TEXTURE_2D, /*level=*/0, gl.RGBA, /*w=*/1, /*h=*/1,
                /*border=*/0, gl.RGBA, gl.UNSIGNED_BYTE,
                new Uint8Array([0, 0, 255, 255]));

  const video = document.querySelector('video');

  draw = function(timestamp) {
    gl.clearColor(Math.sin(timestamp / 100), 1, 1, 1);
    gl.clear(gl.COLOR_BUFFER_BIT);

    if (isVideoLoaded && uvsNeedRecomputation) {
      const newUVs = center_image(video.videoWidth / video.videoHeight,
                                  canvas.width / canvas.height);
      console.log(`new uvs: ${newUVs}`);
      const uvData = [ newUVs[0], newUVs[1],
                       newUVs[2], newUVs[1],
                       newUVs[0], newUVs[3],
                       newUVs[2], newUVs[3] ];
      gl.bindBuffer(gl.ARRAY_BUFFER, uvBuf);
      gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(uvData), gl.STATIC_DRAW);
      uvsNeedRecomputation = false;
    }
    if (isVideoLoaded) {
      gl.texImage2D(gl.TEXTURE_2D, /*level=*/0, gl.RGBA, gl.RGBA,
                    gl.UNSIGNED_BYTE, video);
    }
    gl.drawArrays(gl.TRIANGLE_STRIP, /*offset=*/0, /*count=*/4);
  };
}

// Returns uv coordinates to center an image with one aspect ratio in
// a canvas with a different aspect ratio.
function center_image(image_aspect, canvas_aspect) {
  if (canvas_aspect < image_aspect) {
    const r = canvas_aspect / image_aspect;
    return [(1 - r) / 2, 0,
            (1 + r) / 2, 1];
  }
  const r = image_aspect / canvas_aspect;
  return [0, (1 - r) / 2,
          1, (1 + r) / 2];
}

function animation_frame(timestamp) {
  draw(timestamp);
  requestAnimationFrame(animation_frame);
}

setUpGl();
</script>
