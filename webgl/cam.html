<!doctype html>
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta charset="utf-8">
<title>webcam</title>

<style>
@media(prefers-color-scheme:dark) {
  body {
    color:#E8EAED;
    background:#202124;
  }
}

video {
  display: none;
}

</style>

<video></video>
<canvas width="800" height="600"></canvas>

<script>

let isVideoLoaded = false;

// https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia#width_and_height
// For `video: true`, the built-in camera for my MBP returns a 640x480 stream,
// with a 4:3 aspect ratio. With `video: { height: 720 }`, it instead returns
// a 1280x720 stream, with a 16:9 video. With an external a7iv connected as
// webcam, this gets a 1920x1080 stream (16:9 aspect) with `video: true`.
// But with `video: { height: 720 }`, it gets the 1280x720 stream from the
// built-in camera (!).
// With `video: { height: 1080 }`, this gets 1920x1080 from the built-in
// camera if the a7iv isn't connected, but 1920x1080 from the a7iv if it is
// connected.
// With `video: { height: 2160 }`, this gets 1080x1920 (9:16) (!) from the
// built-in camera no matter if the a7iv is connected or not.
// This is weird in itself, but the non-reliable aspect ratio also means that
// for things not to look stretched on the canvas element, we either have to
// resize it to have an aspect ration matching the video's aspect ratio,
// or clip the the video element a bit on the sides (or bottom/top).
navigator.mediaDevices
  .getUserMedia({ video: true })
  .then((mediaStream) => {
    const video = document.querySelector('video');
    video.srcObject = mediaStream;
    video.onloadedmetadata = () => {
      function gcd(a, b) { return b ? gcd(b, a %b ) : a; }
      const n = gcd(video.videoWidth, video.videoHeight);
      console.log(`video size: ${video.videoWidth} x ${video.videoHeight} ` +
                  `(${video.videoWidth / n}:${video.videoHeight / n})`);
      video.play();
      isVideoLoaded = true;
      requestAnimationFrame(animation_frame);
    };
  })
  .catch((err) => {
    console.error(`${err.name}: ${err.message}`);
  });

function compileShader(gl, shaderType, shaderSource) {
  const shader = gl.createShader(shaderType);
  gl.shaderSource(shader, shaderSource);
  gl.compileShader(shader);
  if (gl.getShaderParameter(shader, gl.COMPILE_STATUS))
    return shader;

  console.error(`failed to compile shader: ${gl.getShaderInfoLog(shader)}`);
  gl.deleteShader(shader);
}

function linkProgram(gl, vertexShader, fragmentShader) {
  const program = gl.createProgram();
  gl.attachShader(program, vertexShader);
  gl.attachShader(program, fragmentShader);
  gl.linkProgram(program);
  if (gl.getProgramParameter(program, gl.LINK_STATUS))
    return program;

  console.error(`failed to link program: ${gl.getProgramInfoLog(program)}`);
  gl.deleteProgram(program);
}

function compileProgram(gl, vertexShaderSource, fragShaderSource) {
  const vertexShader = compileShader(gl, gl.VERTEX_SHADER, vertexShaderSource);
  const fragShader = compileShader(gl, gl.FRAGMENT_SHADER, fragShaderSource);
  return linkProgram(gl, vertexShader, fragShader);
}

let draw;

function setUpGl() {
  const canvas = document.querySelector('canvas');
  const gl = canvas.getContext('webgl2', {alpha: false});
  if (gl === null) {
    console.error('failed to create webgl context');
    return;
  }

  const vertexShaderSource = `#version 300 es
    in vec2 a_position;
    out vec2 v_uv;
    void main() {
      // Map from -1..1 to 0..1 and flip horizontally.
      v_uv = 0.5 * vec2(1.0 - a_position.x, 1.0 + a_position.y);
      gl_Position = vec4(a_position, 0.0, 1.0);
    }
  `;
  const fragmentShaderSource = `#version 300 es
    precision highp float;
    uniform sampler2D u_image;
    in vec2 v_uv;
    out vec4 outColor;
    void main() {
      outColor = texture(u_image, v_uv);
      outColor.rg *= v_uv;
    }
  `;

  const program = compileProgram(gl, vertexShaderSource, fragmentShaderSource);
  const positionAttributeLoc = gl.getAttribLocation(program, 'a_position');
  const imageAttributeLoc = gl.getAttribLocation(program, 'u_image');

  const allCoords = [
    -1, -1,
     1, -1,
    -1,  1,
     1,  1,
  ];
  const positionBuf = gl.createBuffer();
  gl.bindBuffer(gl.ARRAY_BUFFER, positionBuf);
  gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(allCoords), gl.STATIC_DRAW);

  const triVao = gl.createVertexArray();
  gl.bindVertexArray(triVao);
  gl.enableVertexAttribArray(positionAttributeLoc);
  gl.vertexAttribPointer(
      positionAttributeLoc, /*size=*/2, gl.FLOAT, /*normalize=*/false,
      /*stride=*/0,
      /*offset=*/0);

  gl.useProgram(program);

  let texture = gl.createTexture();
  gl.bindTexture(gl.TEXTURE_2D, texture);

  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);

  gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, true);
  gl.texImage2D(gl.TEXTURE_2D, /*level=*/0, gl.RGBA, /*w=*/1, /*h=*/1,
                /*border=*/0, gl.RGBA, gl.UNSIGNED_BYTE,
                new Uint8Array([0, 0, 255, 255]));

  const video = document.querySelector('video');

  draw = function(timestamp) {
    gl.clearColor(Math.sin(timestamp / 100), 1, 1, 1);
    gl.clear(gl.COLOR_BUFFER_BIT);

    if (isVideoLoaded) {
      gl.texImage2D(gl.TEXTURE_2D, /*level=*/0, gl.RGBA, gl.RGBA,
                    gl.UNSIGNED_BYTE, video);
    }
    gl.drawArrays(gl.TRIANGLE_STRIP, /*offset=*/0, /*count=*/4);
  };
  draw(0);
}

function animation_frame(timestamp) {
  draw(timestamp);
  requestAnimationFrame(animation_frame);
}

setUpGl();
</script>
